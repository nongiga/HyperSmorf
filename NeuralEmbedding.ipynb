{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralEmbedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPwIguTbKlbYHsv6B3hq+UB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nongiga/HyperSmorf/blob/main/NeuralEmbedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference paper: https://github.com/geomstats/challenge-iclr-2021/blob/main/gcorso__Neural-Sequence-Distance-Embeddings/Neural_SEED.ipynb"
      ],
      "metadata": {
        "id": "Bx0oQu5FAfAy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AzgZrRomfP86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98a16e18-7db0-4759-9798-a92ca23b5c4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: geomstats in /usr/local/lib/python3.7/dist-packages (2.4.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from geomstats) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from geomstats) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.7/dist-packages (from geomstats) (1.21.5)\n",
            "Requirement already satisfied: joblib>=0.14.1 in /usr/local/lib/python3.7/dist-packages (from geomstats) (1.1.0)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from geomstats) (1.3.5)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.7/dist-packages (from geomstats) (3.5.1)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->geomstats) (3.0.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->geomstats) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->geomstats) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->geomstats) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->geomstats) (1.3.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->geomstats) (4.29.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->geomstats) (21.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->geomstats) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->geomstats) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->geomstats) (3.1.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "clustalw is already the newest version (2.1+lgpl-5).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-470\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.7/dist-packages (1.79)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from biopython) (1.21.5)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.7/dist-packages (0.12.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (57.4.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (0.29.28)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (2.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.2.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.6.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.7.1+cu101 in /usr/local/lib/python3.7/dist-packages (1.7.1+cu101)\n",
            "Requirement already satisfied: torchvision==0.8.2+cu101 in /usr/local/lib/python3.7/dist-packages (0.8.2+cu101)\n",
            "Requirement already satisfied: torchaudio==0.7.2 in /usr/local/lib/python3.7/dist-packages (0.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu101) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu101) (3.10.0.2)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2+cu101) (7.1.2)\n",
            "fatal: destination path 'neural_seed' already exists and is not an empty directory.\n",
            "running build_ext\n",
            "copying build/lib.linux-x86_64-3.7/mst.cpython-37m-x86_64-linux-gnu.so -> \n",
            "running build_ext\n",
            "copying build/lib.linux-x86_64-3.7/unionfind.cpython-37m-x86_64-linux-gnu.so -> \n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.8.0)\n"
          ]
        }
      ],
      "source": [
        "# INITIAL SET UP: ONLY NEED TO RUN ONCE PER RUNTIME\n",
        "\n",
        "!pip3 install geomstats\n",
        "!apt install clustalw\n",
        "!pip install biopython\n",
        "!pip install python-Levenshtein\n",
        "!pip install Cython\n",
        "!pip install networkx\n",
        "!pip install tqdm\n",
        "!pip install gdown\n",
        "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!git clone https://github.com/gcorso/neural_seed.git\n",
        "import os\n",
        "os.chdir(\"neural_seed\")\n",
        "!cd hierarchical_clustering/relaxed/mst; python setup.py build_ext --inplace; cd ../unionfind; python setup.py build_ext --inplace; cd ..; cd ..; cd ..;\n",
        "os.environ['GEOMSTATS_BACKEND'] = 'pytorch'\n",
        "\n",
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# INITIAL SET UP CONTINUED: ONLY NEED ONCE PER RUNTIME\n",
        "import torch\n",
        "import os \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# from geomstats.geometry.poincare_ball import PoincareBall\n",
        "#!TODO: figure out if above step is necessary\n",
        "\n",
        "from edit_distance.train import load_edit_distance_dataset\n",
        "from util.data_handling.data_loader import get_dataloaders\n",
        "from util.ml_and_math.loss_functions import AverageMeter"
      ],
      "metadata": {
        "id": "qCJPu83QALTz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORT DATASET HERE\n",
        "# commented-out portion is reference from neuralseed\n",
        "\n",
        "!wget https://github.com/bhattlab/SupplementaryInformation/raw/master/SmORFinder/datasets.tar.gz\n",
        "!tar -zxvf datasets.tar.gz -C '/content/'\n",
        "\n",
        "# !gdown --id 1yZTOYrnYdW9qRrwHSO5eRc8rYIPEVtY2 # for edit distance approximation\n",
        "# !gdown --id 1hQSHR-oeuS9bDVE6ABHS0SoI4xk3zPnB # for closest string retrieval\n",
        "# !gdown --id 1ukvUI6gUTbcBZEzTVDpskrX8e6EHqVQg # for hierarchical clustering"
      ],
      "metadata": {
        "id": "T9IZ857bATN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "290d190f-3e5a-4cfa-a985-070c9ae90cbe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-27 08:54:06--  https://github.com/bhattlab/SupplementaryInformation/raw/master/SmORFinder/datasets.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/bhattlab/SupplementaryInformation/master/SmORFinder/datasets.tar.gz [following]\n",
            "--2022-02-27 08:54:06--  https://raw.githubusercontent.com/bhattlab/SupplementaryInformation/master/SmORFinder/datasets.tar.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 71158740 (68M) [application/octet-stream]\n",
            "Saving to: ‘datasets.tar.gz.4’\n",
            "\n",
            "datasets.tar.gz.4   100%[===================>]  67.86M   270MB/s    in 0.3s    \n",
            "\n",
            "2022-02-27 08:54:07 (270 MB/s) - ‘datasets.tar.gz.4’ saved [71158740/71158740]\n",
            "\n",
            "datasets/\n",
            "datasets/dataset_FINAL.tsv\n",
            "datasets/dataset_HOLDOUT.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Processing\n",
        "df = pd.read_csv(\"datasets/dataset_FINAL.tsv\", sep='\\t')\n",
        "df.head()\n",
        "\n",
        "# save subset of dataset\n",
        "\n",
        "df_subset=df.sample(10000, random_state=42)\n",
        "df_subset.to_csv('smorf_subset.csv')\n",
        "# get only protein sequence from training subset\n",
        "\n",
        "train=df_subset[df_subset.set=='train']\n",
        "x=train.smorf.tolist()\n",
        "y=train.y=='positive'\n",
        "# encode the protein sequence as a one-hot\n",
        "\n",
        "def onehote(sequence):\n",
        "  seq_array = np.array(list(sequence))\n",
        "  #integer encode the sequence\n",
        "  label_encoder = LabelEncoder()\n",
        "  integer_encoded_seq = label_encoder.fit_transform(seq_array) \n",
        "  #one hot the sequence\n",
        "  onehot_encoder = OneHotEncoder(sparse=False)\n",
        "  #reshape because that's what OneHotEncoder likes\n",
        "  integer_encoded_seq = integer_encoded_seq.reshape(len(integer_encoded_seq), 1)\n",
        "  onehot_encoded_seq = onehot_encoder.fit_transform(integer_encoded_seq)\n",
        "  return onehot_encoded_seq"
      ],
      "metadata": {
        "id": "4v3o3jMDU47k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearEncoder(nn.Module):\n",
        "    \"\"\"  Linear model which simply flattens the sequence and applies a linear transformation. \"\"\"\n",
        "\n",
        "    def __init__(self, len_sequence, embedding_size, alphabet_size=4):\n",
        "        super(LinearEncoder, self).__init__()\n",
        "        self.encoder = nn.Linear(in_features=alphabet_size * len_sequence, \n",
        "                                 out_features=embedding_size)\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        # flatten sequence and apply layer\n",
        "        B = sequence.shape[0]\n",
        "        sequence = sequence.reshape(B, -1)\n",
        "        emb = self.encoder(sequence)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class PairEmbeddingDistance(nn.Module):\n",
        "    \"\"\" Wrapper model for a general encoder, computes pairwise distances and applies projections \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_model, embedding_size, scaling=False):\n",
        "        super(PairEmbeddingDistance, self).__init__()\n",
        "        self.hyperbolic_metric = PoincareBall(embedding_size).metric.dist\n",
        "        self.embedding_model = embedding_model\n",
        "        self.radius = nn.Parameter(torch.Tensor([1e-2]), requires_grad=True)\n",
        "        self.scaling = nn.Parameter(torch.Tensor([1.]), requires_grad=True)\n",
        "\n",
        "    def normalize_embeddings(self, embeddings):\n",
        "        \"\"\" Project embeddings to an hypersphere of a certain radius \"\"\"\n",
        "        min_scale = 1e-7\n",
        "        max_scale = 1 - 1e-3\n",
        "        return F.normalize(embeddings, p=2, dim=1) * self.radius.clamp_min(min_scale).clamp_max(max_scale)\n",
        "\n",
        "    def encode(self, sequence):\n",
        "        \"\"\" Use embedding model and normalization to encode some sequences. \"\"\"\n",
        "        enc_sequence = self.embedding_model(sequence)\n",
        "        enc_sequence = self.normalize_embeddings(enc_sequence)\n",
        "        return enc_sequence\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        # flatten couples\n",
        "        (B, _, N, _) = sequence.shape\n",
        "        sequence = sequence.reshape(2 * B, N, -1)\n",
        "\n",
        "        # encode sequences\n",
        "        enc_sequence = self.encode(sequence)\n",
        "\n",
        "        # compute distances\n",
        "        enc_sequence = enc_sequence.reshape(B, 2, -1)\n",
        "        distance = self.hyperbolic_metric(enc_sequence[:, 0], enc_sequence[:, 1])\n",
        "        distance = distance * self.scaling\n",
        "\n",
        "        return distance"
      ],
      "metadata": {
        "id": "XFd6z6CKAino"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, optimizer, loss, device):\n",
        "    avg_loss = AverageMeter()\n",
        "    model.train()\n",
        "\n",
        "    for sequences, labels in loader:\n",
        "        # move examples to right device\n",
        "        sequences, labels = sequences.to(device), labels.to(device)\n",
        "\n",
        "        # forward propagation\n",
        "        optimizer.zero_grad()\n",
        "        output = model(sequences)\n",
        "\n",
        "        # loss and backpropagation\n",
        "        loss_train = loss(output, labels)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # keep track of average loss\n",
        "        avg_loss.update(loss_train.data.item(), sequences.shape[0])\n",
        "\n",
        "    return avg_loss.avg\n",
        "\n",
        "\n",
        "def test(model, loader, loss, device):\n",
        "    avg_loss = AverageMeter()\n",
        "    model.eval()\n",
        "\n",
        "    for sequences, labels in loader:\n",
        "        # move examples to right device\n",
        "        sequences, labels = sequences.to(device), labels.to(device)\n",
        "\n",
        "        # forward propagation and loss computation\n",
        "        output = model(sequences)\n",
        "        loss_val = loss(output, labels).data.item()\n",
        "        avg_loss.update(loss_val, sequences.shape[0])\n",
        "\n",
        "    return avg_loss.avg"
      ],
      "metadata": {
        "id": "vc_lKEGpAicp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_SIZE = 128\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.manual_seed(2021)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed(2021)\n",
        "\n",
        "# load data\n",
        "datasets = load_edit_distance_dataset('./edit_qiita_large.pkl')\n",
        "loaders = get_dataloaders(datasets, batch_size=128, workers=1)\n",
        "\n",
        "# model, optimizer and loss\n",
        "encoder = LinearEncoder(152, EMBEDDING_SIZE)\n",
        "model = PairEmbeddingDistance(embedding_model=encoder, embedding_size=EMBEDDING_SIZE)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "# training\n",
        "for epoch in range(0, 21):\n",
        "    t = time.time()\n",
        "    loss_train = train(model, loaders['train'], optimizer, loss, device)\n",
        "    loss_val = test(model, loaders['val'], loss, device)\n",
        "\n",
        "    # print progress\n",
        "    if epoch % 5 == 0:\n",
        "        print('Epoch: {:02d}'.format(epoch),\n",
        "              'loss_train: {:.6f}'.format(loss_train),\n",
        "              'loss_val: {:.6f}'.format(loss_val),\n",
        "              'time: {:.4f}s'.format(time.time() - t))\n",
        "      \n",
        "# testing\n",
        "for dset in loaders.keys():\n",
        "    avg_loss = test(model, loaders[dset], loss, device)\n",
        "    print('Final results {}: loss = {:.6f}'.format(dset, avg_loss))"
      ],
      "metadata": {
        "id": "aVeTJJDhAxgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hierarchical_clustering.unsupervised.unsupervised import hierarchical_clustering_testing\n",
        "\n",
        "hierarchical_clustering_testing(encoder_model=model, data_path='./hc_qiita_large_extr.pkl',\n",
        "                                batch_size=128, device=device, distance='hyperbolic')"
      ],
      "metadata": {
        "id": "EXOw4DvmA6z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from multiple_alignment.guide_tree.guide_tree import approximate_guide_trees\n",
        "\n",
        "# performs neighbour joining algorithm on the estimate of the pairwise distance matrix\n",
        "approximate_guide_trees(encoder_model=model, dataset=datasets['test'],\n",
        "                        batch_size=128, device=device, distance='hyperbolic')\n",
        "\n",
        "# Command line clustalw using the tree generated with the previous command. \n",
        "# The substitution matrix and gap penalties are set to simulate the classical edit distance used to train the model \n",
        "!clustalw -infile=\"sequences.fasta\" -dnamatrix=multiple_alignment/guide_tree/matrix.txt -transweight=0 -type='DNA' -gapopen=1 -gapext=1 -gapdist=10000 -usetr"
      ],
      "metadata": {
        "id": "wGztyOt5A9Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Firu0TWTA_3z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}