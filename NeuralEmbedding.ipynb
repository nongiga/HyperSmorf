{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralEmbedding.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP0shV64B1177z9D2qh0nDL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nongiga/HyperSmorf/blob/main/NeuralEmbedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference paper: https://github.com/geomstats/challenge-iclr-2021/blob/main/gcorso__Neural-Sequence-Distance-Embeddings/Neural_SEED.ipynb"
      ],
      "metadata": {
        "id": "Bx0oQu5FAfAy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzgZrRomfP86"
      },
      "outputs": [],
      "source": [
        "# INITIAL SET UP: ONLY NEED TO RUN ONCE PER RUNTIME\n",
        "\n",
        "!pip3 install geomstats\n",
        "!apt install clustalw\n",
        "!pip install biopython\n",
        "!pip install python-Levenshtein\n",
        "!pip install Cython\n",
        "!pip install networkx\n",
        "!pip install tqdm\n",
        "!pip install gdown\n",
        "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!git clone https://github.com/gcorso/neural_seed.git\n",
        "import os\n",
        "os.chdir(\"neural_seed\")\n",
        "!cd hierarchical_clustering/relaxed/mst; python setup.py build_ext --inplace; cd ../unionfind; python setup.py build_ext --inplace; cd ..; cd ..; cd ..;\n",
        "os.environ['GEOMSTATS_BACKEND'] = 'pytorch'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# INITIAL SET UP CONTINUED: ONLY NEED ONCE PER RUNTIME\n",
        "import torch\n",
        "import os \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "# from geomstats.geometry.poincare_ball import PoincareBall\n",
        "#!TODO: figure out if above step is necessary\n",
        "\n",
        "from edit_distance.train import load_edit_distance_dataset\n",
        "from util.data_handling.data_loader import get_dataloaders\n",
        "from util.ml_and_math.loss_functions import AverageMeter"
      ],
      "metadata": {
        "id": "qCJPu83QALTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORT DATASET HERE\n",
        "# commented-out portion is reference from neuralseed\n",
        "\n",
        "# [IMPORT THINGS HERE]\n",
        "\n",
        "# !gdown --id 1yZTOYrnYdW9qRrwHSO5eRc8rYIPEVtY2 # for edit distance approximation\n",
        "# !gdown --id 1hQSHR-oeuS9bDVE6ABHS0SoI4xk3zPnB # for closest string retrieval\n",
        "# !gdown --id 1ukvUI6gUTbcBZEzTVDpskrX8e6EHqVQg # for hierarchical clustering"
      ],
      "metadata": {
        "id": "T9IZ857bATN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearEncoder(nn.Module):\n",
        "    \"\"\"  Linear model which simply flattens the sequence and applies a linear transformation. \"\"\"\n",
        "\n",
        "    def __init__(self, len_sequence, embedding_size, alphabet_size=4):\n",
        "        super(LinearEncoder, self).__init__()\n",
        "        self.encoder = nn.Linear(in_features=alphabet_size * len_sequence, \n",
        "                                 out_features=embedding_size)\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        # flatten sequence and apply layer\n",
        "        B = sequence.shape[0]\n",
        "        sequence = sequence.reshape(B, -1)\n",
        "        emb = self.encoder(sequence)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class PairEmbeddingDistance(nn.Module):\n",
        "    \"\"\" Wrapper model for a general encoder, computes pairwise distances and applies projections \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_model, embedding_size, scaling=False):\n",
        "        super(PairEmbeddingDistance, self).__init__()\n",
        "        self.hyperbolic_metric = PoincareBall(embedding_size).metric.dist\n",
        "        self.embedding_model = embedding_model\n",
        "        self.radius = nn.Parameter(torch.Tensor([1e-2]), requires_grad=True)\n",
        "        self.scaling = nn.Parameter(torch.Tensor([1.]), requires_grad=True)\n",
        "\n",
        "    def normalize_embeddings(self, embeddings):\n",
        "        \"\"\" Project embeddings to an hypersphere of a certain radius \"\"\"\n",
        "        min_scale = 1e-7\n",
        "        max_scale = 1 - 1e-3\n",
        "        return F.normalize(embeddings, p=2, dim=1) * self.radius.clamp_min(min_scale).clamp_max(max_scale)\n",
        "\n",
        "    def encode(self, sequence):\n",
        "        \"\"\" Use embedding model and normalization to encode some sequences. \"\"\"\n",
        "        enc_sequence = self.embedding_model(sequence)\n",
        "        enc_sequence = self.normalize_embeddings(enc_sequence)\n",
        "        return enc_sequence\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        # flatten couples\n",
        "        (B, _, N, _) = sequence.shape\n",
        "        sequence = sequence.reshape(2 * B, N, -1)\n",
        "\n",
        "        # encode sequences\n",
        "        enc_sequence = self.encode(sequence)\n",
        "\n",
        "        # compute distances\n",
        "        enc_sequence = enc_sequence.reshape(B, 2, -1)\n",
        "        distance = self.hyperbolic_metric(enc_sequence[:, 0], enc_sequence[:, 1])\n",
        "        distance = distance * self.scaling\n",
        "\n",
        "        return distance"
      ],
      "metadata": {
        "id": "XFd6z6CKAino"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, optimizer, loss, device):\n",
        "    avg_loss = AverageMeter()\n",
        "    model.train()\n",
        "\n",
        "    for sequences, labels in loader:\n",
        "        # move examples to right device\n",
        "        sequences, labels = sequences.to(device), labels.to(device)\n",
        "\n",
        "        # forward propagation\n",
        "        optimizer.zero_grad()\n",
        "        output = model(sequences)\n",
        "\n",
        "        # loss and backpropagation\n",
        "        loss_train = loss(output, labels)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # keep track of average loss\n",
        "        avg_loss.update(loss_train.data.item(), sequences.shape[0])\n",
        "\n",
        "    return avg_loss.avg\n",
        "\n",
        "\n",
        "def test(model, loader, loss, device):\n",
        "    avg_loss = AverageMeter()\n",
        "    model.eval()\n",
        "\n",
        "    for sequences, labels in loader:\n",
        "        # move examples to right device\n",
        "        sequences, labels = sequences.to(device), labels.to(device)\n",
        "\n",
        "        # forward propagation and loss computation\n",
        "        output = model(sequences)\n",
        "        loss_val = loss(output, labels).data.item()\n",
        "        avg_loss.update(loss_val, sequences.shape[0])\n",
        "\n",
        "    return avg_loss.avg"
      ],
      "metadata": {
        "id": "vc_lKEGpAicp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_SIZE = 128\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.manual_seed(2021)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed(2021)\n",
        "\n",
        "# load data\n",
        "datasets = load_edit_distance_dataset('./edit_qiita_large.pkl')\n",
        "loaders = get_dataloaders(datasets, batch_size=128, workers=1)\n",
        "\n",
        "# model, optimizer and loss\n",
        "encoder = LinearEncoder(152, EMBEDDING_SIZE)\n",
        "model = PairEmbeddingDistance(embedding_model=encoder, embedding_size=EMBEDDING_SIZE)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "# training\n",
        "for epoch in range(0, 21):\n",
        "    t = time.time()\n",
        "    loss_train = train(model, loaders['train'], optimizer, loss, device)\n",
        "    loss_val = test(model, loaders['val'], loss, device)\n",
        "\n",
        "    # print progress\n",
        "    if epoch % 5 == 0:\n",
        "        print('Epoch: {:02d}'.format(epoch),\n",
        "              'loss_train: {:.6f}'.format(loss_train),\n",
        "              'loss_val: {:.6f}'.format(loss_val),\n",
        "              'time: {:.4f}s'.format(time.time() - t))\n",
        "      \n",
        "# testing\n",
        "for dset in loaders.keys():\n",
        "    avg_loss = test(model, loaders[dset], loss, device)\n",
        "    print('Final results {}: loss = {:.6f}'.format(dset, avg_loss))"
      ],
      "metadata": {
        "id": "aVeTJJDhAxgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hierarchical_clustering.unsupervised.unsupervised import hierarchical_clustering_testing\n",
        "\n",
        "hierarchical_clustering_testing(encoder_model=model, data_path='./hc_qiita_large_extr.pkl',\n",
        "                                batch_size=128, device=device, distance='hyperbolic')"
      ],
      "metadata": {
        "id": "EXOw4DvmA6z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from multiple_alignment.guide_tree.guide_tree import approximate_guide_trees\n",
        "\n",
        "# performs neighbour joining algorithm on the estimate of the pairwise distance matrix\n",
        "approximate_guide_trees(encoder_model=model, dataset=datasets['test'],\n",
        "                        batch_size=128, device=device, distance='hyperbolic')\n",
        "\n",
        "# Command line clustalw using the tree generated with the previous command. \n",
        "# The substitution matrix and gap penalties are set to simulate the classical edit distance used to train the model \n",
        "!clustalw -infile=\"sequences.fasta\" -dnamatrix=multiple_alignment/guide_tree/matrix.txt -transweight=0 -type='DNA' -gapopen=1 -gapext=1 -gapdist=10000 -usetr"
      ],
      "metadata": {
        "id": "wGztyOt5A9Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Firu0TWTA_3z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}